{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Text Classification</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import stanza\n",
    "\n",
    "#from preprocess import * \n",
    "from custom_preprocessing import CustomPreProcessing\n",
    "from custom_preprocessing import PreProcessing\n",
    "from class_metric import Metrics\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import itertools\n",
    "from textblob import TextBlob \n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "import string\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Call tqdm to see progress bar with pandas\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Parameters</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part allows you to determine the text column to classify as well as the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"text\"\n",
    "LABEL = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome in this custom preprocessing class\n",
      "        \n",
      "Welcome in the preprocessing\n"
     ]
    }
   ],
   "source": [
    "# ---- Create object to preprocess the text \n",
    "preprocess = CustomPreProcessing()\n",
    "preproc = PreProcessing()\n",
    "Metric = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>List of Models</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results           = False\n",
    "lang                   = False\n",
    "sample                 = True\n",
    "multinomial_naive_bayes= True\n",
    "logistic_regression    = True\n",
    "svm_model              = False\n",
    "sgd                    = True\n",
    "random_forest          = True\n",
    "gradient_boosting      = True\n",
    "xgboost_classifier     = True\n",
    "shallow_network        = True\n",
    "deep_nn                = True\n",
    "rnn                    = True\n",
    "lstm                   = True\n",
    "cnn                    = True\n",
    "gru                    = True\n",
    "cnn_lstm               = True\n",
    "cnn_gru                = True\n",
    "bidirectional_rnn      = True\n",
    "bidirectional_lstm     = True\n",
    "bidirectional_gru      = True\n",
    "rcnn                   = True\n",
    "pre_trained            = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i><h1>Sand Box to Load Data</h1></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sandbox is the working area of your data if it has not been processed before using the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb_sentiment_analysis_dataset(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[(y_train!=1) & (y_train!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[x_train, y_train], index=[\"text\", \"label\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(pd.DataFrame(data=[x_test, y_test], index=[\"text\", \"label\"]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSSIBLE SPOILERS&lt;br /&gt;&lt;br /&gt;The Spy Who Shagg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The long list of \"big\" names in this flick (in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bette Midler showcases her talents and beauty ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great movie when I saw it. Have to say one of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Although it's most certainly politically incor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  POSSIBLE SPOILERS<br /><br />The Spy Who Shagg...     0\n",
       "1  The long list of \"big\" names in this flick (in...     0\n",
       "2  Bette Midler showcases her talents and beauty ...     1\n",
       "3  Great movie when I saw it. Have to say one of ...     1\n",
       "4  Although it's most certainly politically incor...     1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[LABEL]==1) | (df[LABEL]==0)]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABEL].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess.func_remove_upper_case)\n",
    "df[\"text\"] = df[\"text\"].apply(preproc.func_remove_URL)\n",
    "df[\"text\"] = df[\"text\"].apply(preproc.func_remove_html)\n",
    "df[\"text\"] = df[\"text\"].apply(preproc.func_remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i><h1>Sart Pipeline</h1></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lang:\n",
    "    # ---- Language detection of the text\n",
    "    df.loc[:,\"language\"] = df[TEXT].progress_apply(preproc.func_detect_lang_google)\n",
    "    # ---- Extract most frequent language \n",
    "    language = df.language.value_counts().index.tolist()[0]\n",
    "    print(f\"The language most present in the dataset is {language}\")\n",
    "else:\n",
    "    language=\"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Prepare data for ML Classic</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample:\n",
    "    df_save = df.copy()\n",
    "    df = df.sample(5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load stopwords \n",
    "if language==\"fr\":\n",
    "    stop_word = np.loadtxt(\"stopwords-fr/stopwords-fr.txt\", dtype=str)\n",
    "if language==\"en\":\n",
    "    stop_word = np.loadtxt(\"stopwords_en.txt\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:08<00:00, 583.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df.loc[:,TEXT+\"_sw\"] = df.loc[:,TEXT].progress_apply(lambda x : preproc.func_remove_stop_words(x, stop_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[TEXT+\"_sw\"].isnull().sum()>0:\n",
    "    print(\"Empty text\")\n",
    "    df[TEXT+\"_sw\"][df[TEXT+\"_sw\"].isnull()] = \"empty_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABEL].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Machine Learning</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "# ML classic \n",
    "train_x_sw, valid_x_sw, y_train_sw, y_valid_sw = model_selection.train_test_split(df[TEXT+\"_sw\"], df[LABEL], random_state=42, stratify=df[LABEL], test_size=0.2)\n",
    "\n",
    "# For Embeddings\n",
    "train_x, valid_x, y_train, y_valid = model_selection.train_test_split(df[TEXT], df[LABEL], random_state=42, stratify=df[LABEL], test_size=0.2)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_sw = encoder.fit_transform(y_train_sw)\n",
    "valid_y_sw = encoder.fit_transform(y_valid_sw)\n",
    "train_y = encoder.fit_transform(y_train)\n",
    "valid_y = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>Classes Weight</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the class weight with sklearn \n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: 1.0194\tclass: 0\n",
      "Class weight: 0.9814\tclass: 1\n"
     ]
    }
   ],
   "source": [
    "print(*[f'Class weight: {round(i[0],4)}\\tclass: {i[1]}' for i in zip(class_weights, np.unique(y_train))], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset is balanced (ratio=0.962)\n"
     ]
    }
   ],
   "source": [
    "# Determined if the dataset is balanced or imbalanced \n",
    "ratio = np.min(df.label.value_counts()) / np.max(df.label.value_counts())\n",
    "if ratio > 0.1:      # Ratio 1:10 -> limite blanced / imbalanced \n",
    "    balanced = True\n",
    "    print(f\"\\nThe dataset is balanced (ratio={round(ratio, 3)})\")\n",
    "else:\n",
    "    balanced = False\n",
    "    print(f\"\\nThe dataset is imbalanced (ratio={round(ratio, 3)})\")\n",
    "    #from imblearn.over_sampling import ADASYN\n",
    "    # put class for debalanced data \n",
    "    # in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Save Unique Labels</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the unique label corresponding to their encoding correspondance\n",
    "labels = df[LABEL].unique()\n",
    "test=pd.DataFrame(data=np.transpose([labels,encoder.fit_transform(labels)]), columns=[\"labels\", \"encoding\"]).sort_values(by=[\"encoding\"])\n",
    "labels=test.labels.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One-Hot encoding (CountVectorizing)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 46.9 ms, total: 1.69 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df[TEXT]+\"_sw\")\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x_sw)\n",
    "xvalid_count =  count_vect.transform(valid_x_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain_tfidf.toarray()[0][xtrain_tfidf.toarray()[0]  >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word level tf-idf done\n",
      "ngram level tf-idf done\n",
      "characters level tf-idf done\n",
      "CPU times: user 20.8 s, sys: 1.19 s, total: 22 s\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "tfidf_vect.fit(df[TEXT])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x_sw)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x_sw)\n",
    "print(\"word level tf-idf done\")\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect_ngram.fit(df[TEXT])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x_sw)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x_sw)\n",
    "print(\"ngram level tf-idf done\")\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',  ngram_range=(2,3), max_features=10000) #token_pattern=r'\\w{1,}',\n",
    "tfidf_vect_ngram_chars.fit(df[TEXT])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x_sw) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x_sw) \n",
    "print(\"characters level tf-idf done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Pre-Trained model fastText</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.02 s, sys: 11.3 s, total: 17.3 s\n",
      "Wall time: 17.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if language==\"fr\":\n",
    "    pretrained = fasttext.FastText.load_model('fastText/cc.fr.300.bin')\n",
    "if language==\"en\":\n",
    "    pretrained = fasttext.FastText.load_model('fastText/crawl-300d-2M-subword.bin')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word Embeddings</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43321/43321 [00:00<00:00, 45221.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.02 s, sys: 109 ms, total: 3.12 s\n",
      "Wall time: 3.26 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a tokenizer \n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df[TEXT])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=300)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=300)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "words = []\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = pretrained.get_word_vector(word) #embeddings_index.get(word)\n",
    "    words.append(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words[1], embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(clf, x, y, name='classifier', cv=5, fit_params=None):\n",
    "    \n",
    "    scoring = {'acc': 'accuracy',\n",
    "           'balanced_accuracy':'balanced_accuracy',\n",
    "           'prec': 'precision',\n",
    "           'recall': 'recall',\n",
    "           'f1-score':'f1',\n",
    "           'tp': make_scorer(tp), 'tn': make_scorer(tn),\n",
    "           'fp': make_scorer(fp), 'fn': make_scorer(fn),\n",
    "            'cohens_kappa':make_scorer(cohen_kappa_score),\n",
    "            'matthews_corrcoef':make_scorer(matthews_corrcoef),\n",
    "              \"roc_auc\":make_scorer(roc_auc_score)}\n",
    "    #if clf==XGBClassifier():\n",
    "    scores = cross_validate(clf, x, y, scoring=scoring,\n",
    "                         cv=cv, return_train_score=False, n_jobs=-1,  fit_params=fit_params)\n",
    "    \n",
    "    index = []\n",
    "    value = []\n",
    "    index.append(\"Model\")\n",
    "    value.append(name)\n",
    "    for i in scores:\n",
    "        if i == \"estimator\":\n",
    "            continue\n",
    "        for j in enumerate(scores[i]):\n",
    "            index.append(i+\"_cv\"+str(j[0]+1))\n",
    "            value.append(j[1])\n",
    "        #if any(x in i for x in scoring.keys()):\n",
    "        \n",
    "        index.append(i+\"_mean\")\n",
    "        value.append(np.mean(scores[i]))\n",
    "        index.append(i+\"_std\")\n",
    "        value.append(np.std(scores[i]))\n",
    "        \n",
    "    return pd.DataFrame(data=value, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Multinomial Naive Bayes</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 172 ms, sys: 4.41 s, total: 4.58 s\n",
      "Wall time: 6.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if multinomial_naive_bayes:\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_count,train_y_sw, name='NB_Count_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf,train_y_sw, name='NB_WordLevel_TF-IDF', cv=5))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram,train_y_sw, name='NB_N-Gram_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars,train_y_sw, name='NB_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Logistic Regression</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 266 ms, sys: 0 ns, total: 266 ms\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if logistic_regression:\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_count,train_y_sw, name='LR_Count_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf,train_y_sw, name='LR_WordLevel_TF-IDF', cv=5))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf_ngram,train_y_sw, name='LR_N-Gram_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf_ngram_chars,train_y_sw, name='LR_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>SVM</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 19.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if svm_model:\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_count,train_y_sw, name='SVM_Count_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf,train_y_sw, name='SVM_WordLevel_TF-IDF', cv=5))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf_ngram,train_y_sw, name='SVM_N-Gram_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(svm.SVC(), xtrain_tfidf_ngram_chars,train_y_sw, name='SVM_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>RandomForest</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 62.5 ms, total: 109 ms\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_count,train_y_sw, name='RF_Count_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.5 ms, sys: 0 ns, total: 62.5 ms\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf,train_y_sw, name='RF_WordLevel_TF-IDF', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 15.6 ms, total: 62.5 ms\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf_ngram,train_y_sw, name='RF_N-Gram_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.8 ms, sys: 15.6 ms, total: 109 ms\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), xtrain_tfidf_ngram_chars,train_y_sw, name='RF_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.1 ms, sys: 0 ns, total: 78.1 ms\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if random_forest:\n",
    "    df_results = df_results.append(report(ensemble.RandomForestClassifier(bootstrap=True,min_impurity_decrease=1e-7,n_jobs=-1, random_state=42), train_seq_x,train_y, name='RF_Words', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Stochastic Descent</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 250 ms, sys: 78.1 ms, total: 328 ms\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if sgd:\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_count,train_y_sw, name='SGD_Count_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf,train_y_sw, name='SGD_WordLevel_TF-IDF', cv=5))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf_ngram,train_y_sw, name='SGD_N-Gram_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), xtrain_tfidf_ngram_chars,train_y_sw, name='SGD_CharLevel_Vectors', cv=5))\n",
    "    df_results = df_results.append(report(SGDClassifier(loss='modified_huber', max_iter=1000, tol=1e-3,   n_iter_no_change=10, early_stopping=True, n_jobs=-1 ), train_seq_x,train_y, name='SGD_Words', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Gradient Boosting</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 62.5 ms, total: 109 ms\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_count,train_y_sw, name='GB_Count_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.5 ms, sys: 31.2 ms, total: 93.8 ms\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf,train_y_sw, name='GB_WordLevel_TF-IDF', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 62.5 ms, total: 78.1 ms\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf_ngram,train_y_sw, name='GB_N-Gram_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.5 ms, sys: 31.2 ms, total: 93.8 ms\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), xtrain_tfidf_ngram_chars,train_y_sw, name='GB_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 46.9 ms, total: 46.9 ms\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gradient_boosting:\n",
    "    df_results = df_results.append(report(ensemble.GradientBoostingClassifier(n_estimators=1000,\n",
    "                                               validation_fraction=0.2,\n",
    "                                               n_iter_no_change=10, tol=0.01,\n",
    "                                               random_state=0, verbose=0 ), train_seq_x,train_y, name='GB_CharLevel_Vectors', cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>XGBoost Classifier</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the XGBoost have early stopping implemented with 10 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.5 ms, sys: 1.08 s, total: 1.14 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_count, valid_y_sw)]}\n",
    "    df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_count,train_y_sw, name='XGB_Count_Vectors', cv=5, fit_params=fit_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.2 ms, sys: 1.69 s, total: 1.72 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf, valid_y_sw)]}\n",
    "    df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf,train_y_sw, name='XGB_WordLevel_TF-IDF', cv=5, fit_params=fit_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.1 ms, sys: 15.6 ms, total: 93.8 ms\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf_ngram, valid_y_sw)]}\n",
    "    df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram,train_y_sw, name='XGB_N-Gram_Vectors', cv=5, fit_params=fit_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 62.5 ms, total: 109 ms\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(xvalid_tfidf_ngram_chars, valid_y_sw)]}\n",
    "    df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), xtrain_tfidf_ngram_chars,train_y_sw, name='XGB_CharLevel_Vectors', cv=5, fit_params=fit_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.1 ms, sys: 31.2 ms, total: 109 ms\n",
      "Wall time: 5.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if xgboost_classifier:\n",
    "    fit_params={'early_stopping_rounds':10,\\\n",
    "                         'eval_set':[(valid_seq_x,valid_y)]}\n",
    "    df_results = df_results.append(report(XGBClassifier(n_estimators=1000, subsample=0.8), train_seq_x,train_y, name='XGB_Words', cv=5, fit_params=fit_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_acc_mean</th>\n",
       "      <th>test_acc_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>test_prec_mean</th>\n",
       "      <th>test_prec_std</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "      <th>test_f1-score_mean</th>\n",
       "      <th>test_f1-score_std</th>\n",
       "      <th>test_cohens_kappa_mean</th>\n",
       "      <th>test_cohens_kappa_std</th>\n",
       "      <th>test_matthews_corrcoef_mean</th>\n",
       "      <th>test_matthews_corrcoef_std</th>\n",
       "      <th>test_roc_auc_mean</th>\n",
       "      <th>test_roc_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.84425</td>\n",
       "      <td>0.00979796</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.00987974</td>\n",
       "      <td>0.85776</td>\n",
       "      <td>0.0148784</td>\n",
       "      <td>0.832679</td>\n",
       "      <td>0.0119426</td>\n",
       "      <td>0.844921</td>\n",
       "      <td>0.00933715</td>\n",
       "      <td>0.688565</td>\n",
       "      <td>0.0196391</td>\n",
       "      <td>0.689048</td>\n",
       "      <td>0.0197066</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.00987974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.83125</td>\n",
       "      <td>0.00890926</td>\n",
       "      <td>0.831661</td>\n",
       "      <td>0.00888021</td>\n",
       "      <td>0.851671</td>\n",
       "      <td>0.0118938</td>\n",
       "      <td>0.810105</td>\n",
       "      <td>0.0166339</td>\n",
       "      <td>0.83022</td>\n",
       "      <td>0.00965036</td>\n",
       "      <td>0.662697</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.831661</td>\n",
       "      <td>0.00888021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0120312</td>\n",
       "      <td>0.849242</td>\n",
       "      <td>0.0119993</td>\n",
       "      <td>0.844876</td>\n",
       "      <td>0.0109466</td>\n",
       "      <td>0.86311</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.853854</td>\n",
       "      <td>0.0119668</td>\n",
       "      <td>0.69877</td>\n",
       "      <td>0.0240643</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>0.0241805</td>\n",
       "      <td>0.849242</td>\n",
       "      <td>0.0119993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.84925</td>\n",
       "      <td>0.00963717</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.00977927</td>\n",
       "      <td>0.843826</td>\n",
       "      <td>0.015964</td>\n",
       "      <td>0.864581</td>\n",
       "      <td>0.0105832</td>\n",
       "      <td>0.853936</td>\n",
       "      <td>0.00827072</td>\n",
       "      <td>0.698246</td>\n",
       "      <td>0.0193793</td>\n",
       "      <td>0.698716</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.00977927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.828219</td>\n",
       "      <td>0.00899646</td>\n",
       "      <td>0.841285</td>\n",
       "      <td>0.00950077</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0106341</td>\n",
       "      <td>0.828675</td>\n",
       "      <td>0.00909323</td>\n",
       "      <td>0.656069</td>\n",
       "      <td>0.0179905</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.0179992</td>\n",
       "      <td>0.828219</td>\n",
       "      <td>0.00899646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model test_acc_mean test_acc_std  \\\n",
       "0   NB_WordLevel_TF-IDF       0.84425   0.00979796   \n",
       "0      NB_Count_Vectors       0.83125   0.00890926   \n",
       "0      LR_Count_Vectors        0.8495    0.0120312   \n",
       "0  SGD_WordLevel_TF-IDF       0.84925   0.00963717   \n",
       "0   RF_WordLevel_TF-IDF         0.828        0.009   \n",
       "\n",
       "  test_balanced_accuracy_mean test_balanced_accuracy_std test_prec_mean  \\\n",
       "0                    0.844484                 0.00987974        0.85776   \n",
       "0                    0.831661                 0.00888021       0.851671   \n",
       "0                    0.849242                  0.0119993       0.844876   \n",
       "0                    0.848965                 0.00977927       0.843826   \n",
       "0                    0.828219                 0.00899646       0.841285   \n",
       "\n",
       "  test_prec_std test_recall_mean test_recall_std test_f1-score_mean  \\\n",
       "0     0.0148784         0.832679       0.0119426           0.844921   \n",
       "0     0.0118938         0.810105       0.0166339            0.83022   \n",
       "0     0.0109466          0.86311        0.015549           0.853854   \n",
       "0      0.015964         0.864581       0.0105832           0.853936   \n",
       "0    0.00950077         0.816482       0.0106341           0.828675   \n",
       "\n",
       "  test_f1-score_std test_cohens_kappa_mean test_cohens_kappa_std  \\\n",
       "0        0.00933715               0.688565             0.0196391   \n",
       "0        0.00965036               0.662697              0.017784   \n",
       "0         0.0119668                0.69877             0.0240643   \n",
       "0        0.00827072               0.698246             0.0193793   \n",
       "0        0.00909323               0.656069             0.0179905   \n",
       "\n",
       "  test_matthews_corrcoef_mean test_matthews_corrcoef_std test_roc_auc_mean  \\\n",
       "0                    0.689048                  0.0197066          0.844484   \n",
       "0                    0.663726                   0.017619          0.831661   \n",
       "0                    0.699015                  0.0241805          0.849242   \n",
       "0                    0.698716                   0.018907          0.848965   \n",
       "0                      0.6564                  0.0179992          0.828219   \n",
       "\n",
       "  test_roc_auc_std  \n",
       "0       0.00987974  \n",
       "0       0.00888021  \n",
       "0        0.0119993  \n",
       "0       0.00977927  \n",
       "0       0.00899646  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[[ \"Model\",\"test_acc_mean\",\"test_acc_std\", \n",
    "                        \"test_balanced_accuracy_mean\",\"test_balanced_accuracy_std\", \n",
    "                       \"test_prec_mean\", \"test_prec_std\", \n",
    "                        \"test_recall_mean\",\"test_recall_std\", \n",
    "                       \"test_f1-score_mean\", \"test_f1-score_std\", \n",
    "                       \"test_cohens_kappa_mean\", \"test_cohens_kappa_std\", \"test_matthews_corrcoef_mean\",\"test_matthews_corrcoef_std\", \n",
    "                       \"test_roc_auc_mean\", \"test_roc_auc_std\"]].sort_values(by=[\"test_prec_mean\", \"test_recall_mean\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Deep Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cohen’s kappa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [cohen_kappa_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score) computes [Cohen’s kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) statistic. This measure is intended to compare labelings by different human annotators, not a classifier versus a ground truth.\n",
    "\n",
    "The kappa score (see docstring) is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels).\n",
    "\n",
    "Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems (except by manually computing a per-label score) and not for more than two annotators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Balanced Accuracy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the balanced accuracy\n",
    "\n",
    "The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n",
    "\n",
    "The best value is 1 and the worst value is 0 when adjusted=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Early Stopping, Model saving, Class weight configuration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=3)\n",
    "check_p = tf.keras.callbacks.ModelCheckpoint(\"save_models/model.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_w = {}\n",
    "for i in zip(range(len(class_weights)), class_weights):\n",
    "    class_w[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate_NN(model, X, y, X_test, y_test, callbacks,name=\"NN\", fit_params=None, scoring=None, n_splits=5):\n",
    "    #print(model.__class__.__name__)\n",
    "    # ---- Parameters initialisation\n",
    "    seed = 42\n",
    "    i = 1\n",
    "    np.random.seed(seed)\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    fit_time, score_time, acc, acc_balanced = [], [], [], []\n",
    "    pred_weight, recall_weight, f1_score_weight = [], [], []\n",
    "    tp_, tn_, fp_, fn_ = [], [], [], []\n",
    "    cohen_kappa_, matthews_corrcoef_, roc_auc_ = [], [], []\n",
    "    index = [\"Model\"]\n",
    "    results = [name]\n",
    "    # ---- Loop on k-fold for cross-valisation\n",
    "    for train, test in kfold.split(X, y):\n",
    "        # create model\n",
    "        print(f\"k-fold : {i}\")\n",
    "        fit_start = time.time()\n",
    "        _model = model\n",
    "        _model.fit(X[train], y[train],\n",
    "                        epochs=1000, callbacks=[callbacks],\n",
    "                        validation_split=0.2, verbose=False)\n",
    "        \n",
    "        fit_end = time.time() - fit_start\n",
    "\n",
    "        _acc = _model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "        score_start = time.time()\n",
    "        y_pred = (model.predict(X_test)>0.5).astype(int)\n",
    "        score_end = time.time() - score_start\n",
    "\n",
    "        # ---- save each metric\n",
    "        fit_time.append(fit_end)\n",
    "        index.append('fit_time_cv'+str(i))\n",
    "        results.append(fit_end)\n",
    "        \n",
    "        score_time.append(score_end)\n",
    "        index.append('score_time_cv'+str(i))\n",
    "        results.append(score_end)\n",
    "        \n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "        index.append( 'test_acc_cv'+str(i))\n",
    "        results.append(acc[-1])\n",
    "        \n",
    "        acc_balanced.append(balanced_accuracy_score(y_test, y_pred))\n",
    "        index.append('test_balanced_accuracy_cv'+str(i))\n",
    "        results.append(acc_balanced[-1])\n",
    "        \n",
    "        pred_weight.append(precision_score(y_test, y_pred))\n",
    "        index.append('test_prec_cv'+str(i))\n",
    "        results.append(pred_weight[-1])\n",
    "        \n",
    "        recall_weight.append(recall_score(y_test, y_pred))\n",
    "        results.append(recall_weight[-1])\n",
    "        index.append('test_recall_cv'+str(i))\n",
    "        \n",
    "        f1_score_weight.append(f1_score(y_test, y_pred) )\n",
    "        index.append('test_f1-score_cv'+str(i))\n",
    "        results.append(f1_score_weight[-1])\n",
    "        \n",
    "        tp_.append(tp(y_test, y_pred))\n",
    "        index.append('test_tp_cv'+str(i))\n",
    "        results.append(tp_[-1])\n",
    "        \n",
    "        tn_.append(tn(y_test, y_pred))\n",
    "        index.append('test_tn_cv'+str(i))\n",
    "        results.append(tn_[-1])\n",
    "        \n",
    "        fp_.append(fp(y_test, y_pred))\n",
    "        index.append('test_fp_cv'+str(i))\n",
    "        results.append(fp_[-1])\n",
    "        \n",
    "        fn_.append(fn(y_test, y_pred))\n",
    "        index.append('test_fn_cv'+str(i))\n",
    "        results.append(fn_[-1])\n",
    "        \n",
    "        cohen_kappa_.append(cohen_kappa_score(y_test, y_pred))\n",
    "        index.append('test_cohens_kappa_cv'+str(i))\n",
    "        results.append(cohen_kappa_[-1])\n",
    "        \n",
    "        matthews_corrcoef_.append(matthews_corrcoef(y_test, y_pred))\n",
    "        index.append('test_matthews_corrcoef_cv'+str(i))\n",
    "        results.append(matthews_corrcoef_[-1])\n",
    "        \n",
    "        roc_auc_.append(roc_auc_score(y_test, y_pred))\n",
    "        index.append('test_roc_auc_cv'+str(i))\n",
    "        results.append(roc_auc_[-1])\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    index.extend([ \"fit_time_mean\",\"fit_time_std\",'score_time_mean', 'score_time_std',\n",
    "                  \"test_acc_mean\",\"test_acc_std\", \n",
    "                        \"test_balanced_accuracy_mean\",\"test_balanced_accuracy_std\", \n",
    "                       \"test_prec_mean\", \"test_prec_std\", \n",
    "                        \"test_recall_mean\",\"test_recall_std\", \n",
    "                       \"test_f1-score_mean\", \"test_f1-score_std\", 'test_tp_mean', 'test_tp_std', 'test_tn_mean', \n",
    "                  'test_tn_std', 'test_fp_mean', 'test_fp_std','test_fn_mean', 'test_fn_std',\n",
    "                       \"test_cohens_kappa_mean\", \"test_cohens_kappa_std\", \"test_matthews_corrcoef_mean\",\"test_matthews_corrcoef_std\", \n",
    "                       \"test_roc_auc_mean\", \"test_roc_auc_std\"])\n",
    "\n",
    "    results.extend([np.mean(fit_time), np.std(fit_time),np.mean(score_time), np.std(score_time),np.mean(acc), np.std(acc), np.mean(acc_balanced), \n",
    "     np.std(acc_balanced), np.mean(pred_weight),np.std(pred_weight),\n",
    "     np.mean(recall_weight),np.std(recall_weight),np.mean(f1_score_weight), np.std(f1_score_weight), np.mean(tp_),np.std(tp_),np.mean(tn_),np.std(tn_),\n",
    "                    np.mean(fp_),np.std(fp_), np.mean(fn_),np.std(fn_), np.mean(cohen_kappa_),np.std(cohen_kappa_),\n",
    "     np.mean(matthews_corrcoef_),np.std(matthews_corrcoef_),np.mean(roc_auc_),np.std(roc_auc_)])\n",
    "\n",
    "    #print(list(zip(index, results)))\n",
    "    \n",
    "    return pd.DataFrame(results, index=index).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Shallow Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 16)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      \n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 1h 22min 19s, sys: 7min 22s, total: 1h 29min 42s\n",
      "Wall time: 35min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if shallow_network:\n",
    "    df_results = df_results.append(cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"Shallow_NN_WE\", scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = cross_validate_NN(shallow_neural_networks(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"Shallow_NN_WE\", scoring=None, n_splits=5)\n",
    "#c[[\"test_acc_cv1\", \"test_recall_cv1\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation='relu'),\n",
    "      keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 4min 52s, sys: 34.2 s, total: 5min 26s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"Deep_NN_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Neural Networks variation 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(16, activation='relu'),\n",
    "      keras.layers.Dense(16, activation='relu'),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 2min 20s, sys: 13.3 s, total: 2min 33s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var1(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"Deep_NN_var1_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Neural Networks variation 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_neural_networks_var2(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "      embedded,\n",
    "      keras.layers.GlobalAveragePooling1D(),\n",
    "      keras.layers.Dense(32, activation='relu'),\n",
    "      keras.layers.Dense(16, activation='relu'),\n",
    "      keras.layers.Dense(1  if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 1min 47s, sys: 11.2 s, total: 1min 59s\n",
      "Wall time: 47.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if deep_nn:\n",
    "    df_results = df_results.append(cross_validate_NN(deep_neural_networks_var2(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"Deep_NN_var2_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurent Neural Network - RNN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 50)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(40, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(40),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 52min 2s, sys: 6min 56s, total: 58min 59s\n",
      "Wall time: 19min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rnn_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"RNN_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Convolutional Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) +1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(100, 5, activation='relu'), # padding='same'\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(64, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.Conv1D(32, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.GlobalMaxPooling1D(),\n",
    "\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 40min 59s, sys: 5min 26s, total: 46min 26s\n",
      "Wall time: 15min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rnn_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"CNN_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Neural Network – LSTM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) +1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.LSTM(32),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 20min 50s, sys: 1min 45s, total: 22min 35s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_lstm_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"LSTM_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN – LSTM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.LSTM(32),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 30min 12s, sys: 1min 54s, total: 32min 6s\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn_lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_cnn_lstm_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es,name=\"CNN_LSTM_WE\", scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN – GRU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.MaxPooling1D(pool_size=4),\n",
    "    keras.layers.GRU(32),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 26min 6s, sys: 1min 12s, total: 27min 19s\n",
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if cnn_gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_cnn_gru_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"CNN_GRU_WE\", scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Neural Network – GRU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.layers.GRU(\n",
    "    units, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None,\n",
    "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
    "    recurrent_constraint=None, bias_constraint=None, dropout=0.0,\n",
    "    recurrent_dropout=0.0, implementation=2, return_sequences=False,\n",
    "    return_state=False, go_backwards=False, stateful=False, unroll=False,\n",
    "    time_major=False, reset_after=True, **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.GRU(32),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 41min 18s, sys: 3min 36s, total: 44min 55s\n",
      "Wall time: 14min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_gru_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"GRU_WE\", scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bidirectional RNN</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_rnn_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.SimpleRNN(32)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 5\n",
      "CPU times: user 1h 18min 16s, sys: 9min 21s, total: 1h 27min 38s\n",
      "Wall time: 26min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_rnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_rnn_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"BiRNN_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bidirectional LSTM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_lstm_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 45min 13s, sys: 1min 19s, total: 46min 33s\n",
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_lstm:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_lstm_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"BiLSTM_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bidirectional GRU</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirec_gru_model(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 59min 28s, sys: 2min 15s, total: 1h 1min 43s\n",
      "Wall time: 17min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if bidirectional_gru:\n",
    "    df_results = df_results.append(cross_validate_NN(create_bidirec_gru_model(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"BiGRU_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Convolutional Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn(X, word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300,input_length=X.shape[1], weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True)),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 1h 1min 25s, sys: 2min 21s, total: 1h 3min 46s\n",
      "Wall time: 17min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn(train_seq_x, word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"RCNN_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Convolutional Neural Network variation 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var1(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32,return_sequences=True)),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 55min 1s, sys: 1min 34s, total: 56min 36s\n",
      "Wall time: 16min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var1(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"RCNN_var1_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Convulational Neural Network variation 2</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var2(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True)),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n",
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 2h 10min 54s, sys: 6min 19s, total: 2h 17min 14s\n",
      "Wall time: 38min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var2(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"RCNN_var2_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Recurrent Convulational Neural Network variation 3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn_var3(word_index, label=labels, embedding_matrix=embedding_matrix, pre_trained=False):\n",
    "    if pre_trained==False:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 100)\n",
    "    else:\n",
    "        embedded = keras.layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    embedded,\n",
    "    keras.layers.SpatialDropout1D(0.3),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(32,return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32,return_sequences=True)),\n",
    "    keras.layers.Convolution1D(32, 3, activation=\"relu\"),\n",
    "    keras.layers.GlobalMaxPool1D(),\n",
    "    keras.layers.Dense(25, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(1 if len(label)<=2 else len(label), activation='sigmoid' if len(label)<=2 else \"softmax\")])\n",
    "\n",
    "    if len(label)==2:\n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 1\n",
      "k-fold : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chris/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold : 3\n",
      "k-fold : 4\n",
      "k-fold : 5\n",
      "CPU times: user 50min 29s, sys: 2min 32s, total: 53min 1s\n",
      "Wall time: 14min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rcnn:\n",
    "    df_results = df_results.append(cross_validate_NN(create_rcnn_var3(word_index, pre_trained=pre_trained), train_seq_x, train_y, valid_seq_x, valid_y, es, name=\"RCNN_var3_WE\",scoring=None, n_splits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Results</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_acc_mean</th>\n",
       "      <th>test_acc_std</th>\n",
       "      <th>test_balanced_accuracy_mean</th>\n",
       "      <th>test_balanced_accuracy_std</th>\n",
       "      <th>test_prec_mean</th>\n",
       "      <th>test_prec_std</th>\n",
       "      <th>test_recall_mean</th>\n",
       "      <th>test_recall_std</th>\n",
       "      <th>test_f1-score_mean</th>\n",
       "      <th>test_f1-score_std</th>\n",
       "      <th>test_cohens_kappa_mean</th>\n",
       "      <th>test_cohens_kappa_std</th>\n",
       "      <th>test_matthews_corrcoef_mean</th>\n",
       "      <th>test_matthews_corrcoef_std</th>\n",
       "      <th>test_roc_auc_mean</th>\n",
       "      <th>test_roc_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RCNN_WE</td>\n",
       "      <td>0.8248</td>\n",
       "      <td>0.0238445</td>\n",
       "      <td>0.826875</td>\n",
       "      <td>0.0225728</td>\n",
       "      <td>0.921394</td>\n",
       "      <td>0.036202</td>\n",
       "      <td>0.723137</td>\n",
       "      <td>0.0868554</td>\n",
       "      <td>0.805326</td>\n",
       "      <td>0.0388524</td>\n",
       "      <td>0.651076</td>\n",
       "      <td>0.0466568</td>\n",
       "      <td>0.671749</td>\n",
       "      <td>0.0313675</td>\n",
       "      <td>0.826875</td>\n",
       "      <td>0.0225728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Shallow_NN_WE</td>\n",
       "      <td>0.7724</td>\n",
       "      <td>0.030748</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.874746</td>\n",
       "      <td>0.00343647</td>\n",
       "      <td>0.646275</td>\n",
       "      <td>0.0700919</td>\n",
       "      <td>0.741085</td>\n",
       "      <td>0.0492035</td>\n",
       "      <td>0.54713</td>\n",
       "      <td>0.0603521</td>\n",
       "      <td>0.568289</td>\n",
       "      <td>0.0487082</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.029947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Deep_NN_var1_WE</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.00670522</td>\n",
       "      <td>0.812709</td>\n",
       "      <td>0.00612512</td>\n",
       "      <td>0.874506</td>\n",
       "      <td>0.0224713</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.0385033</td>\n",
       "      <td>0.798841</td>\n",
       "      <td>0.0140079</td>\n",
       "      <td>0.623457</td>\n",
       "      <td>0.0129221</td>\n",
       "      <td>0.632775</td>\n",
       "      <td>0.00743012</td>\n",
       "      <td>0.812709</td>\n",
       "      <td>0.00612512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>CNN_LSTM_WE</td>\n",
       "      <td>0.8292</td>\n",
       "      <td>0.0416625</td>\n",
       "      <td>0.83006</td>\n",
       "      <td>0.0401371</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>0.0431336</td>\n",
       "      <td>0.787059</td>\n",
       "      <td>0.126871</td>\n",
       "      <td>0.818868</td>\n",
       "      <td>0.0660651</td>\n",
       "      <td>0.659149</td>\n",
       "      <td>0.0817655</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>0.0625067</td>\n",
       "      <td>0.83006</td>\n",
       "      <td>0.0401371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Deep_NN_var2_WE</td>\n",
       "      <td>0.8134</td>\n",
       "      <td>0.0157048</td>\n",
       "      <td>0.81485</td>\n",
       "      <td>0.0151005</td>\n",
       "      <td>0.873564</td>\n",
       "      <td>0.0121224</td>\n",
       "      <td>0.742353</td>\n",
       "      <td>0.0461947</td>\n",
       "      <td>0.801492</td>\n",
       "      <td>0.0240338</td>\n",
       "      <td>0.627823</td>\n",
       "      <td>0.0308348</td>\n",
       "      <td>0.636273</td>\n",
       "      <td>0.0241465</td>\n",
       "      <td>0.81485</td>\n",
       "      <td>0.0151005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Deep_NN_WE</td>\n",
       "      <td>0.8166</td>\n",
       "      <td>0.00233238</td>\n",
       "      <td>0.817939</td>\n",
       "      <td>0.00214471</td>\n",
       "      <td>0.872539</td>\n",
       "      <td>0.0184199</td>\n",
       "      <td>0.75098</td>\n",
       "      <td>0.0224251</td>\n",
       "      <td>0.806693</td>\n",
       "      <td>0.00616396</td>\n",
       "      <td>0.634089</td>\n",
       "      <td>0.00450131</td>\n",
       "      <td>0.641114</td>\n",
       "      <td>0.00511518</td>\n",
       "      <td>0.817939</td>\n",
       "      <td>0.00214471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB_WordLevel_TF-IDF</td>\n",
       "      <td>0.84425</td>\n",
       "      <td>0.00979796</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.00987974</td>\n",
       "      <td>0.85776</td>\n",
       "      <td>0.0148784</td>\n",
       "      <td>0.832679</td>\n",
       "      <td>0.0119426</td>\n",
       "      <td>0.844921</td>\n",
       "      <td>0.00933715</td>\n",
       "      <td>0.688565</td>\n",
       "      <td>0.0196391</td>\n",
       "      <td>0.689048</td>\n",
       "      <td>0.0197066</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.00987974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_Count_Vectors</td>\n",
       "      <td>0.83125</td>\n",
       "      <td>0.00890926</td>\n",
       "      <td>0.831661</td>\n",
       "      <td>0.00888021</td>\n",
       "      <td>0.851671</td>\n",
       "      <td>0.0118938</td>\n",
       "      <td>0.810105</td>\n",
       "      <td>0.0166339</td>\n",
       "      <td>0.83022</td>\n",
       "      <td>0.00965036</td>\n",
       "      <td>0.662697</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.017619</td>\n",
       "      <td>0.831661</td>\n",
       "      <td>0.00888021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR_Count_Vectors</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0120312</td>\n",
       "      <td>0.849242</td>\n",
       "      <td>0.0119993</td>\n",
       "      <td>0.844876</td>\n",
       "      <td>0.0109466</td>\n",
       "      <td>0.86311</td>\n",
       "      <td>0.015549</td>\n",
       "      <td>0.853854</td>\n",
       "      <td>0.0119668</td>\n",
       "      <td>0.69877</td>\n",
       "      <td>0.0240643</td>\n",
       "      <td>0.699015</td>\n",
       "      <td>0.0241805</td>\n",
       "      <td>0.849242</td>\n",
       "      <td>0.0119993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SGD_WordLevel_TF-IDF</td>\n",
       "      <td>0.84925</td>\n",
       "      <td>0.00963717</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.00977927</td>\n",
       "      <td>0.843826</td>\n",
       "      <td>0.015964</td>\n",
       "      <td>0.864581</td>\n",
       "      <td>0.0105832</td>\n",
       "      <td>0.853936</td>\n",
       "      <td>0.00827072</td>\n",
       "      <td>0.698246</td>\n",
       "      <td>0.0193793</td>\n",
       "      <td>0.698716</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.848965</td>\n",
       "      <td>0.00977927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RF_WordLevel_TF-IDF</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.828219</td>\n",
       "      <td>0.00899646</td>\n",
       "      <td>0.841285</td>\n",
       "      <td>0.00950077</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>0.0106341</td>\n",
       "      <td>0.828675</td>\n",
       "      <td>0.00909323</td>\n",
       "      <td>0.656069</td>\n",
       "      <td>0.0179905</td>\n",
       "      <td>0.6564</td>\n",
       "      <td>0.0179992</td>\n",
       "      <td>0.828219</td>\n",
       "      <td>0.00899646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RF_Count_Vectors</td>\n",
       "      <td>0.83675</td>\n",
       "      <td>0.0162711</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>0.0163817</td>\n",
       "      <td>0.838237</td>\n",
       "      <td>0.019824</td>\n",
       "      <td>0.842504</td>\n",
       "      <td>0.0129731</td>\n",
       "      <td>0.840301</td>\n",
       "      <td>0.0150679</td>\n",
       "      <td>0.673342</td>\n",
       "      <td>0.0326309</td>\n",
       "      <td>0.673457</td>\n",
       "      <td>0.0326426</td>\n",
       "      <td>0.836641</td>\n",
       "      <td>0.0163817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BiGRU_WE</td>\n",
       "      <td>0.7894</td>\n",
       "      <td>0.0439163</td>\n",
       "      <td>0.790264</td>\n",
       "      <td>0.0440962</td>\n",
       "      <td>0.836922</td>\n",
       "      <td>0.0736009</td>\n",
       "      <td>0.747059</td>\n",
       "      <td>0.111113</td>\n",
       "      <td>0.78115</td>\n",
       "      <td>0.0527234</td>\n",
       "      <td>0.579401</td>\n",
       "      <td>0.0876957</td>\n",
       "      <td>0.595043</td>\n",
       "      <td>0.0827983</td>\n",
       "      <td>0.790264</td>\n",
       "      <td>0.0440962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR_WordLevel_TF-IDF</td>\n",
       "      <td>0.8555</td>\n",
       "      <td>0.0150955</td>\n",
       "      <td>0.854735</td>\n",
       "      <td>0.0152261</td>\n",
       "      <td>0.833671</td>\n",
       "      <td>0.0193051</td>\n",
       "      <td>0.895491</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.863368</td>\n",
       "      <td>0.0134542</td>\n",
       "      <td>0.710475</td>\n",
       "      <td>0.030313</td>\n",
       "      <td>0.712728</td>\n",
       "      <td>0.0298079</td>\n",
       "      <td>0.854735</td>\n",
       "      <td>0.0152261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SGD_Count_Vectors</td>\n",
       "      <td>0.83375</td>\n",
       "      <td>0.0080234</td>\n",
       "      <td>0.833525</td>\n",
       "      <td>0.00815279</td>\n",
       "      <td>0.831456</td>\n",
       "      <td>0.0134781</td>\n",
       "      <td>0.845444</td>\n",
       "      <td>0.0111925</td>\n",
       "      <td>0.838265</td>\n",
       "      <td>0.00699441</td>\n",
       "      <td>0.667266</td>\n",
       "      <td>0.0161462</td>\n",
       "      <td>0.667583</td>\n",
       "      <td>0.0159159</td>\n",
       "      <td>0.833525</td>\n",
       "      <td>0.00815279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR_CharLevel_Vectors</td>\n",
       "      <td>0.82275</td>\n",
       "      <td>0.00514782</td>\n",
       "      <td>0.822276</td>\n",
       "      <td>0.00520515</td>\n",
       "      <td>0.81271</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>0.847901</td>\n",
       "      <td>0.0156569</td>\n",
       "      <td>0.829747</td>\n",
       "      <td>0.00519258</td>\n",
       "      <td>0.645079</td>\n",
       "      <td>0.0103395</td>\n",
       "      <td>0.646058</td>\n",
       "      <td>0.0104081</td>\n",
       "      <td>0.822276</td>\n",
       "      <td>0.00520515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>SGD_CharLevel_Vectors</td>\n",
       "      <td>0.80775</td>\n",
       "      <td>0.00826892</td>\n",
       "      <td>0.807483</td>\n",
       "      <td>0.00812435</td>\n",
       "      <td>0.805807</td>\n",
       "      <td>0.0168824</td>\n",
       "      <td>0.821899</td>\n",
       "      <td>0.0334926</td>\n",
       "      <td>0.813074</td>\n",
       "      <td>0.0110926</td>\n",
       "      <td>0.615214</td>\n",
       "      <td>0.0164226</td>\n",
       "      <td>0.616535</td>\n",
       "      <td>0.0163255</td>\n",
       "      <td>0.807483</td>\n",
       "      <td>0.00812435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>CNN_WE</td>\n",
       "      <td>0.7064</td>\n",
       "      <td>0.0455746</td>\n",
       "      <td>0.708315</td>\n",
       "      <td>0.0426989</td>\n",
       "      <td>0.804919</td>\n",
       "      <td>0.089045</td>\n",
       "      <td>0.612549</td>\n",
       "      <td>0.220501</td>\n",
       "      <td>0.657847</td>\n",
       "      <td>0.118456</td>\n",
       "      <td>0.415445</td>\n",
       "      <td>0.0871257</td>\n",
       "      <td>0.455309</td>\n",
       "      <td>0.0597158</td>\n",
       "      <td>0.708315</td>\n",
       "      <td>0.0426989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>XGB_Count_Vectors</td>\n",
       "      <td>0.80925</td>\n",
       "      <td>0.00722842</td>\n",
       "      <td>0.808713</td>\n",
       "      <td>0.00732536</td>\n",
       "      <td>0.798731</td>\n",
       "      <td>0.0139051</td>\n",
       "      <td>0.837091</td>\n",
       "      <td>0.0175754</td>\n",
       "      <td>0.817218</td>\n",
       "      <td>0.00698901</td>\n",
       "      <td>0.618001</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>0.619185</td>\n",
       "      <td>0.0141174</td>\n",
       "      <td>0.808713</td>\n",
       "      <td>0.00732536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>GRU_WE</td>\n",
       "      <td>0.7398</td>\n",
       "      <td>0.137854</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>0.140914</td>\n",
       "      <td>0.787881</td>\n",
       "      <td>0.142985</td>\n",
       "      <td>0.771373</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>0.7512</td>\n",
       "      <td>0.120073</td>\n",
       "      <td>0.477112</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.487356</td>\n",
       "      <td>0.277744</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>0.140914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GB_Count_Vectors</td>\n",
       "      <td>0.8045</td>\n",
       "      <td>0.0142872</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>0.0143071</td>\n",
       "      <td>0.785296</td>\n",
       "      <td>0.0143042</td>\n",
       "      <td>0.848379</td>\n",
       "      <td>0.0156295</td>\n",
       "      <td>0.815565</td>\n",
       "      <td>0.0133732</td>\n",
       "      <td>0.608246</td>\n",
       "      <td>0.0286429</td>\n",
       "      <td>0.610406</td>\n",
       "      <td>0.0286675</td>\n",
       "      <td>0.803649</td>\n",
       "      <td>0.0143071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GB_WordLevel_TF-IDF</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.00566789</td>\n",
       "      <td>0.803021</td>\n",
       "      <td>0.00589433</td>\n",
       "      <td>0.781645</td>\n",
       "      <td>0.010682</td>\n",
       "      <td>0.854265</td>\n",
       "      <td>0.00695619</td>\n",
       "      <td>0.816252</td>\n",
       "      <td>0.00349516</td>\n",
       "      <td>0.607139</td>\n",
       "      <td>0.0115433</td>\n",
       "      <td>0.610064</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.803021</td>\n",
       "      <td>0.00589433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>XGB_WordLevel_TF-IDF</td>\n",
       "      <td>0.79175</td>\n",
       "      <td>0.0130528</td>\n",
       "      <td>0.791131</td>\n",
       "      <td>0.0132908</td>\n",
       "      <td>0.780583</td>\n",
       "      <td>0.018398</td>\n",
       "      <td>0.82336</td>\n",
       "      <td>0.0118592</td>\n",
       "      <td>0.801231</td>\n",
       "      <td>0.0105735</td>\n",
       "      <td>0.582884</td>\n",
       "      <td>0.0263458</td>\n",
       "      <td>0.584084</td>\n",
       "      <td>0.0256784</td>\n",
       "      <td>0.791131</td>\n",
       "      <td>0.0132908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GB_CharLevel_Vectors</td>\n",
       "      <td>0.78925</td>\n",
       "      <td>0.00960469</td>\n",
       "      <td>0.788653</td>\n",
       "      <td>0.0095421</td>\n",
       "      <td>0.778155</td>\n",
       "      <td>0.00948384</td>\n",
       "      <td>0.820423</td>\n",
       "      <td>0.0195021</td>\n",
       "      <td>0.798587</td>\n",
       "      <td>0.0103004</td>\n",
       "      <td>0.57791</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.579067</td>\n",
       "      <td>0.0197852</td>\n",
       "      <td>0.788653</td>\n",
       "      <td>0.0095421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RF_CharLevel_Vectors</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.0147352</td>\n",
       "      <td>0.773892</td>\n",
       "      <td>0.0145199</td>\n",
       "      <td>0.777416</td>\n",
       "      <td>0.0106896</td>\n",
       "      <td>0.779689</td>\n",
       "      <td>0.0303584</td>\n",
       "      <td>0.778283</td>\n",
       "      <td>0.0174456</td>\n",
       "      <td>0.547825</td>\n",
       "      <td>0.0292852</td>\n",
       "      <td>0.548231</td>\n",
       "      <td>0.0294851</td>\n",
       "      <td>0.773892</td>\n",
       "      <td>0.0145199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NB_CharLevel_Vectors</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.0204389</td>\n",
       "      <td>0.792905</td>\n",
       "      <td>0.020566</td>\n",
       "      <td>0.769632</td>\n",
       "      <td>0.0219619</td>\n",
       "      <td>0.850837</td>\n",
       "      <td>0.0164828</td>\n",
       "      <td>0.808111</td>\n",
       "      <td>0.0180025</td>\n",
       "      <td>0.586998</td>\n",
       "      <td>0.0410719</td>\n",
       "      <td>0.590584</td>\n",
       "      <td>0.0403927</td>\n",
       "      <td>0.792905</td>\n",
       "      <td>0.020566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>XGB_CharLevel_Vectors</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.00868188</td>\n",
       "      <td>0.772624</td>\n",
       "      <td>0.00873717</td>\n",
       "      <td>0.769227</td>\n",
       "      <td>0.0108374</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.0141402</td>\n",
       "      <td>0.780557</td>\n",
       "      <td>0.00830063</td>\n",
       "      <td>0.545567</td>\n",
       "      <td>0.0174372</td>\n",
       "      <td>0.546003</td>\n",
       "      <td>0.0175165</td>\n",
       "      <td>0.772624</td>\n",
       "      <td>0.00873717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NB_N-Gram_Vectors</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.0139777</td>\n",
       "      <td>0.728197</td>\n",
       "      <td>0.0139397</td>\n",
       "      <td>0.740256</td>\n",
       "      <td>0.0148014</td>\n",
       "      <td>0.718349</td>\n",
       "      <td>0.0210341</td>\n",
       "      <td>0.728991</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.456127</td>\n",
       "      <td>0.0278991</td>\n",
       "      <td>0.456508</td>\n",
       "      <td>0.0278473</td>\n",
       "      <td>0.728197</td>\n",
       "      <td>0.0139397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR_N-Gram_Vectors</td>\n",
       "      <td>0.72125</td>\n",
       "      <td>0.00890926</td>\n",
       "      <td>0.720811</td>\n",
       "      <td>0.00901459</td>\n",
       "      <td>0.718774</td>\n",
       "      <td>0.0115304</td>\n",
       "      <td>0.744359</td>\n",
       "      <td>0.00516768</td>\n",
       "      <td>0.731297</td>\n",
       "      <td>0.00705717</td>\n",
       "      <td>0.441917</td>\n",
       "      <td>0.0179485</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.0177146</td>\n",
       "      <td>0.720811</td>\n",
       "      <td>0.00901459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RNN_WE</td>\n",
       "      <td>0.6884</td>\n",
       "      <td>0.0905905</td>\n",
       "      <td>0.690116</td>\n",
       "      <td>0.0877003</td>\n",
       "      <td>0.715683</td>\n",
       "      <td>0.077473</td>\n",
       "      <td>0.604314</td>\n",
       "      <td>0.238341</td>\n",
       "      <td>0.631584</td>\n",
       "      <td>0.202399</td>\n",
       "      <td>0.379861</td>\n",
       "      <td>0.175481</td>\n",
       "      <td>0.386408</td>\n",
       "      <td>0.170787</td>\n",
       "      <td>0.690116</td>\n",
       "      <td>0.0877003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CNN_GRU_WE</td>\n",
       "      <td>0.7618</td>\n",
       "      <td>0.136955</td>\n",
       "      <td>0.764958</td>\n",
       "      <td>0.133473</td>\n",
       "      <td>0.714952</td>\n",
       "      <td>0.358258</td>\n",
       "      <td>0.607059</td>\n",
       "      <td>0.308572</td>\n",
       "      <td>0.654732</td>\n",
       "      <td>0.328326</td>\n",
       "      <td>0.528362</td>\n",
       "      <td>0.266279</td>\n",
       "      <td>0.536624</td>\n",
       "      <td>0.269682</td>\n",
       "      <td>0.764958</td>\n",
       "      <td>0.133473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RCNN_var3_WE</td>\n",
       "      <td>0.7004</td>\n",
       "      <td>0.114194</td>\n",
       "      <td>0.704474</td>\n",
       "      <td>0.110685</td>\n",
       "      <td>0.713229</td>\n",
       "      <td>0.366491</td>\n",
       "      <td>0.500784</td>\n",
       "      <td>0.309823</td>\n",
       "      <td>0.563207</td>\n",
       "      <td>0.296869</td>\n",
       "      <td>0.407107</td>\n",
       "      <td>0.221012</td>\n",
       "      <td>0.448446</td>\n",
       "      <td>0.233283</td>\n",
       "      <td>0.704474</td>\n",
       "      <td>0.110685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RF_N-Gram_Vectors</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>0.00183712</td>\n",
       "      <td>0.701117</td>\n",
       "      <td>0.00167203</td>\n",
       "      <td>0.701364</td>\n",
       "      <td>0.00190621</td>\n",
       "      <td>0.721295</td>\n",
       "      <td>0.0102358</td>\n",
       "      <td>0.711141</td>\n",
       "      <td>0.00418997</td>\n",
       "      <td>0.402456</td>\n",
       "      <td>0.00347732</td>\n",
       "      <td>0.402688</td>\n",
       "      <td>0.00363142</td>\n",
       "      <td>0.701117</td>\n",
       "      <td>0.00167203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SGD_N-Gram_Vectors</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.00953939</td>\n",
       "      <td>0.672474</td>\n",
       "      <td>0.00973502</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.017905</td>\n",
       "      <td>0.700687</td>\n",
       "      <td>0.0320761</td>\n",
       "      <td>0.68563</td>\n",
       "      <td>0.0113456</td>\n",
       "      <td>0.345222</td>\n",
       "      <td>0.0193275</td>\n",
       "      <td>0.346415</td>\n",
       "      <td>0.01985</td>\n",
       "      <td>0.672474</td>\n",
       "      <td>0.00973502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>XGB_N-Gram_Vectors</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.00782624</td>\n",
       "      <td>0.686204</td>\n",
       "      <td>0.00799347</td>\n",
       "      <td>0.672636</td>\n",
       "      <td>0.00982383</td>\n",
       "      <td>0.754176</td>\n",
       "      <td>0.0206554</td>\n",
       "      <td>0.710835</td>\n",
       "      <td>0.00823452</td>\n",
       "      <td>0.373286</td>\n",
       "      <td>0.0159061</td>\n",
       "      <td>0.37654</td>\n",
       "      <td>0.0156226</td>\n",
       "      <td>0.686204</td>\n",
       "      <td>0.00799347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LSTM_WE</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.116055</td>\n",
       "      <td>0.723261</td>\n",
       "      <td>0.112165</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.328921</td>\n",
       "      <td>0.610196</td>\n",
       "      <td>0.321462</td>\n",
       "      <td>0.620276</td>\n",
       "      <td>0.311484</td>\n",
       "      <td>0.446171</td>\n",
       "      <td>0.224188</td>\n",
       "      <td>0.457865</td>\n",
       "      <td>0.229394</td>\n",
       "      <td>0.723261</td>\n",
       "      <td>0.112165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GB_N-Gram_Vectors</td>\n",
       "      <td>0.67325</td>\n",
       "      <td>0.0170404</td>\n",
       "      <td>0.670308</td>\n",
       "      <td>0.0174249</td>\n",
       "      <td>0.639712</td>\n",
       "      <td>0.0175325</td>\n",
       "      <td>0.825328</td>\n",
       "      <td>0.0397798</td>\n",
       "      <td>0.719984</td>\n",
       "      <td>0.0147736</td>\n",
       "      <td>0.342524</td>\n",
       "      <td>0.0348301</td>\n",
       "      <td>0.360414</td>\n",
       "      <td>0.0327149</td>\n",
       "      <td>0.670308</td>\n",
       "      <td>0.0174249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RCNN_var1_WE</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.111553</td>\n",
       "      <td>0.637067</td>\n",
       "      <td>0.108846</td>\n",
       "      <td>0.610314</td>\n",
       "      <td>0.317706</td>\n",
       "      <td>0.433725</td>\n",
       "      <td>0.290457</td>\n",
       "      <td>0.47791</td>\n",
       "      <td>0.291703</td>\n",
       "      <td>0.273011</td>\n",
       "      <td>0.217051</td>\n",
       "      <td>0.293217</td>\n",
       "      <td>0.21273</td>\n",
       "      <td>0.637067</td>\n",
       "      <td>0.108846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GB_CharLevel_Vectors</td>\n",
       "      <td>0.53275</td>\n",
       "      <td>0.00721976</td>\n",
       "      <td>0.532194</td>\n",
       "      <td>0.00708121</td>\n",
       "      <td>0.539897</td>\n",
       "      <td>0.00657063</td>\n",
       "      <td>0.560331</td>\n",
       "      <td>0.0234955</td>\n",
       "      <td>0.549732</td>\n",
       "      <td>0.0132659</td>\n",
       "      <td>0.0644407</td>\n",
       "      <td>0.0141835</td>\n",
       "      <td>0.0645503</td>\n",
       "      <td>0.0142397</td>\n",
       "      <td>0.532194</td>\n",
       "      <td>0.00708121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>XGB_Words</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.0139553</td>\n",
       "      <td>0.516931</td>\n",
       "      <td>0.0142689</td>\n",
       "      <td>0.526761</td>\n",
       "      <td>0.0142256</td>\n",
       "      <td>0.521602</td>\n",
       "      <td>0.0175391</td>\n",
       "      <td>0.523857</td>\n",
       "      <td>0.0100611</td>\n",
       "      <td>0.033836</td>\n",
       "      <td>0.0285392</td>\n",
       "      <td>0.0338504</td>\n",
       "      <td>0.0285656</td>\n",
       "      <td>0.516931</td>\n",
       "      <td>0.0142689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RF_Words</td>\n",
       "      <td>0.50875</td>\n",
       "      <td>0.0186916</td>\n",
       "      <td>0.508376</td>\n",
       "      <td>0.0185061</td>\n",
       "      <td>0.517246</td>\n",
       "      <td>0.0176271</td>\n",
       "      <td>0.528461</td>\n",
       "      <td>0.0347442</td>\n",
       "      <td>0.522531</td>\n",
       "      <td>0.0245666</td>\n",
       "      <td>0.0167729</td>\n",
       "      <td>0.0370432</td>\n",
       "      <td>0.0168308</td>\n",
       "      <td>0.0370997</td>\n",
       "      <td>0.508376</td>\n",
       "      <td>0.0185061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BiLSTM_WE</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.146579</td>\n",
       "      <td>0.674206</td>\n",
       "      <td>0.142535</td>\n",
       "      <td>0.516208</td>\n",
       "      <td>0.423299</td>\n",
       "      <td>0.423922</td>\n",
       "      <td>0.34826</td>\n",
       "      <td>0.463665</td>\n",
       "      <td>0.378734</td>\n",
       "      <td>0.347174</td>\n",
       "      <td>0.284027</td>\n",
       "      <td>0.355054</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>0.674206</td>\n",
       "      <td>0.142535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SGD_Words</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.0197579</td>\n",
       "      <td>0.503311</td>\n",
       "      <td>0.0176497</td>\n",
       "      <td>0.507868</td>\n",
       "      <td>0.0207686</td>\n",
       "      <td>0.492759</td>\n",
       "      <td>0.141843</td>\n",
       "      <td>0.490957</td>\n",
       "      <td>0.0916898</td>\n",
       "      <td>0.0067629</td>\n",
       "      <td>0.0352466</td>\n",
       "      <td>0.00598898</td>\n",
       "      <td>0.0368196</td>\n",
       "      <td>0.503311</td>\n",
       "      <td>0.0176497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>BiRNN_WE</td>\n",
       "      <td>0.5726</td>\n",
       "      <td>0.0929271</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.0913936</td>\n",
       "      <td>0.39556</td>\n",
       "      <td>0.335775</td>\n",
       "      <td>0.456863</td>\n",
       "      <td>0.404804</td>\n",
       "      <td>0.402707</td>\n",
       "      <td>0.330353</td>\n",
       "      <td>0.149633</td>\n",
       "      <td>0.182421</td>\n",
       "      <td>0.162689</td>\n",
       "      <td>0.186538</td>\n",
       "      <td>0.574962</td>\n",
       "      <td>0.0913936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model test_acc_mean test_acc_std  \\\n",
       "41                RCNN_WE        0.8248    0.0238445   \n",
       "28          Shallow_NN_WE        0.7724     0.030748   \n",
       "30        Deep_NN_var1_WE        0.8112   0.00670522   \n",
       "35            CNN_LSTM_WE        0.8292    0.0416625   \n",
       "31        Deep_NN_var2_WE        0.8134    0.0157048   \n",
       "29             Deep_NN_WE        0.8166   0.00233238   \n",
       "1     NB_WordLevel_TF-IDF       0.84425   0.00979796   \n",
       "0        NB_Count_Vectors       0.83125   0.00890926   \n",
       "4        LR_Count_Vectors        0.8495    0.0120312   \n",
       "14   SGD_WordLevel_TF-IDF       0.84925   0.00963717   \n",
       "9     RF_WordLevel_TF-IDF         0.828        0.009   \n",
       "8        RF_Count_Vectors       0.83675    0.0162711   \n",
       "40               BiGRU_WE        0.7894    0.0439163   \n",
       "5     LR_WordLevel_TF-IDF        0.8555    0.0150955   \n",
       "13      SGD_Count_Vectors       0.83375    0.0080234   \n",
       "7    LR_CharLevel_Vectors       0.82275   0.00514782   \n",
       "16  SGD_CharLevel_Vectors       0.80775   0.00826892   \n",
       "33                 CNN_WE        0.7064    0.0455746   \n",
       "23      XGB_Count_Vectors       0.80925   0.00722842   \n",
       "37                 GRU_WE        0.7398     0.137854   \n",
       "18       GB_Count_Vectors        0.8045    0.0142872   \n",
       "19    GB_WordLevel_TF-IDF         0.804   0.00566789   \n",
       "24   XGB_WordLevel_TF-IDF       0.79175    0.0130528   \n",
       "21   GB_CharLevel_Vectors       0.78925   0.00960469   \n",
       "11   RF_CharLevel_Vectors         0.774    0.0147352   \n",
       "3    NB_CharLevel_Vectors         0.794    0.0204389   \n",
       "26  XGB_CharLevel_Vectors         0.773   0.00868188   \n",
       "2       NB_N-Gram_Vectors         0.728    0.0139777   \n",
       "6       LR_N-Gram_Vectors       0.72125   0.00890926   \n",
       "32                 RNN_WE        0.6884    0.0905905   \n",
       "36             CNN_GRU_WE        0.7618     0.136955   \n",
       "44           RCNN_var3_WE        0.7004     0.114194   \n",
       "10      RF_N-Gram_Vectors        0.7015   0.00183712   \n",
       "15     SGD_N-Gram_Vectors         0.673   0.00953939   \n",
       "25     XGB_N-Gram_Vectors        0.6875   0.00782624   \n",
       "34                LSTM_WE         0.721     0.116055   \n",
       "20      GB_N-Gram_Vectors       0.67325    0.0170404   \n",
       "42           RCNN_var1_WE         0.633     0.111553   \n",
       "22   GB_CharLevel_Vectors       0.53275   0.00721976   \n",
       "27              XGB_Words         0.517    0.0139553   \n",
       "12               RF_Words       0.50875    0.0186916   \n",
       "39              BiLSTM_WE        0.6692     0.146579   \n",
       "17              SGD_Words         0.503    0.0197579   \n",
       "38               BiRNN_WE        0.5726    0.0929271   \n",
       "\n",
       "   test_balanced_accuracy_mean test_balanced_accuracy_std test_prec_mean  \\\n",
       "41                    0.826875                  0.0225728       0.921394   \n",
       "28                    0.774974                   0.029947       0.874746   \n",
       "30                    0.812709                 0.00612512       0.874506   \n",
       "35                     0.83006                  0.0401371       0.873832   \n",
       "31                     0.81485                  0.0151005       0.873564   \n",
       "29                    0.817939                 0.00214471       0.872539   \n",
       "1                     0.844484                 0.00987974        0.85776   \n",
       "0                     0.831661                 0.00888021       0.851671   \n",
       "4                     0.849242                  0.0119993       0.844876   \n",
       "14                    0.848965                 0.00977927       0.843826   \n",
       "9                     0.828219                 0.00899646       0.841285   \n",
       "8                     0.836641                  0.0163817       0.838237   \n",
       "40                    0.790264                  0.0440962       0.836922   \n",
       "5                     0.854735                  0.0152261       0.833671   \n",
       "13                    0.833525                 0.00815279       0.831456   \n",
       "7                     0.822276                 0.00520515        0.81271   \n",
       "16                    0.807483                 0.00812435       0.805807   \n",
       "33                    0.708315                  0.0426989       0.804919   \n",
       "23                    0.808713                 0.00732536       0.798731   \n",
       "37                    0.739156                   0.140914       0.787881   \n",
       "18                    0.803649                  0.0143071       0.785296   \n",
       "19                    0.803021                 0.00589433       0.781645   \n",
       "24                    0.791131                  0.0132908       0.780583   \n",
       "21                    0.788653                  0.0095421       0.778155   \n",
       "11                    0.773892                  0.0145199       0.777416   \n",
       "3                     0.792905                   0.020566       0.769632   \n",
       "26                    0.772624                 0.00873717       0.769227   \n",
       "2                     0.728197                  0.0139397       0.740256   \n",
       "6                     0.720811                 0.00901459       0.718774   \n",
       "32                    0.690116                  0.0877003       0.715683   \n",
       "36                    0.764958                   0.133473       0.714952   \n",
       "44                    0.704474                   0.110685       0.713229   \n",
       "10                    0.701117                 0.00167203       0.701364   \n",
       "15                    0.672474                 0.00973502       0.672658   \n",
       "25                    0.686204                 0.00799347       0.672636   \n",
       "34                    0.723261                   0.112165       0.647727   \n",
       "20                    0.670308                  0.0174249       0.639712   \n",
       "42                    0.637067                   0.108846       0.610314   \n",
       "22                    0.532194                 0.00708121       0.539897   \n",
       "27                    0.516931                  0.0142689       0.526761   \n",
       "12                    0.508376                  0.0185061       0.517246   \n",
       "39                    0.674206                   0.142535       0.516208   \n",
       "17                    0.503311                  0.0176497       0.507868   \n",
       "38                    0.574962                  0.0913936        0.39556   \n",
       "\n",
       "   test_prec_std test_recall_mean test_recall_std test_f1-score_mean  \\\n",
       "41      0.036202         0.723137       0.0868554           0.805326   \n",
       "28    0.00343647         0.646275       0.0700919           0.741085   \n",
       "30     0.0224713         0.737255       0.0385033           0.798841   \n",
       "35     0.0431336         0.787059        0.126871           0.818868   \n",
       "31     0.0121224         0.742353       0.0461947           0.801492   \n",
       "29     0.0184199          0.75098       0.0224251           0.806693   \n",
       "1      0.0148784         0.832679       0.0119426           0.844921   \n",
       "0      0.0118938         0.810105       0.0166339            0.83022   \n",
       "4      0.0109466          0.86311        0.015549           0.853854   \n",
       "14      0.015964         0.864581       0.0105832           0.853936   \n",
       "9     0.00950077         0.816482       0.0106341           0.828675   \n",
       "8       0.019824         0.842504       0.0129731           0.840301   \n",
       "40     0.0736009         0.747059        0.111113            0.78115   \n",
       "5      0.0193051         0.895491        0.012446           0.863368   \n",
       "13     0.0134781         0.845444       0.0111925           0.838265   \n",
       "7       0.011356         0.847901       0.0156569           0.829747   \n",
       "16     0.0168824         0.821899       0.0334926           0.813074   \n",
       "33      0.089045         0.612549        0.220501           0.657847   \n",
       "23     0.0139051         0.837091       0.0175754           0.817218   \n",
       "37      0.142985         0.771373        0.190132             0.7512   \n",
       "18     0.0143042         0.848379       0.0156295           0.815565   \n",
       "19      0.010682         0.854265      0.00695619           0.816252   \n",
       "24      0.018398          0.82336       0.0118592           0.801231   \n",
       "21    0.00948384         0.820423       0.0195021           0.798587   \n",
       "11     0.0106896         0.779689       0.0303584           0.778283   \n",
       "3      0.0219619         0.850837       0.0164828           0.808111   \n",
       "26     0.0108374         0.792453       0.0141402           0.780557   \n",
       "2      0.0148014         0.718349       0.0210341           0.728991   \n",
       "6      0.0115304         0.744359      0.00516768           0.731297   \n",
       "32      0.077473         0.604314        0.238341           0.631584   \n",
       "36      0.358258         0.607059        0.308572           0.654732   \n",
       "44      0.366491         0.500784        0.309823           0.563207   \n",
       "10    0.00190621         0.721295       0.0102358           0.711141   \n",
       "15      0.017905         0.700687       0.0320761            0.68563   \n",
       "25    0.00982383         0.754176       0.0206554           0.710835   \n",
       "34      0.328921         0.610196        0.321462           0.620276   \n",
       "20     0.0175325         0.825328       0.0397798           0.719984   \n",
       "42      0.317706         0.433725        0.290457            0.47791   \n",
       "22    0.00657063         0.560331       0.0234955           0.549732   \n",
       "27     0.0142256         0.521602       0.0175391           0.523857   \n",
       "12     0.0176271         0.528461       0.0347442           0.522531   \n",
       "39      0.423299         0.423922         0.34826           0.463665   \n",
       "17     0.0207686         0.492759        0.141843           0.490957   \n",
       "38      0.335775         0.456863        0.404804           0.402707   \n",
       "\n",
       "   test_f1-score_std test_cohens_kappa_mean test_cohens_kappa_std  \\\n",
       "41         0.0388524               0.651076             0.0466568   \n",
       "28         0.0492035                0.54713             0.0603521   \n",
       "30         0.0140079               0.623457             0.0129221   \n",
       "35         0.0660651               0.659149             0.0817655   \n",
       "31         0.0240338               0.627823             0.0308348   \n",
       "29        0.00616396               0.634089            0.00450131   \n",
       "1         0.00933715               0.688565             0.0196391   \n",
       "0         0.00965036               0.662697              0.017784   \n",
       "4          0.0119668                0.69877             0.0240643   \n",
       "14        0.00827072               0.698246             0.0193793   \n",
       "9         0.00909323               0.656069             0.0179905   \n",
       "8          0.0150679               0.673342             0.0326309   \n",
       "40         0.0527234               0.579401             0.0876957   \n",
       "5          0.0134542               0.710475              0.030313   \n",
       "13        0.00699441               0.667266             0.0161462   \n",
       "7         0.00519258               0.645079             0.0103395   \n",
       "16         0.0110926               0.615214             0.0164226   \n",
       "33          0.118456               0.415445             0.0871257   \n",
       "23        0.00698901               0.618001              0.014532   \n",
       "37          0.120073               0.477112              0.281715   \n",
       "18         0.0133732               0.608246             0.0286429   \n",
       "19        0.00349516               0.607139             0.0115433   \n",
       "24         0.0105735               0.582884             0.0263458   \n",
       "21         0.0103004                0.57791              0.019187   \n",
       "11         0.0174456               0.547825             0.0292852   \n",
       "3          0.0180025               0.586998             0.0410719   \n",
       "26        0.00830063               0.545567             0.0174372   \n",
       "2           0.015134               0.456127             0.0278991   \n",
       "6         0.00705717               0.441917             0.0179485   \n",
       "32          0.202399               0.379861              0.175481   \n",
       "36          0.328326               0.528362              0.266279   \n",
       "44          0.296869               0.407107              0.221012   \n",
       "10        0.00418997               0.402456            0.00347732   \n",
       "15         0.0113456               0.345222             0.0193275   \n",
       "25        0.00823452               0.373286             0.0159061   \n",
       "34          0.311484               0.446171              0.224188   \n",
       "20         0.0147736               0.342524             0.0348301   \n",
       "42          0.291703               0.273011              0.217051   \n",
       "22         0.0132659              0.0644407             0.0141835   \n",
       "27         0.0100611               0.033836             0.0285392   \n",
       "12         0.0245666              0.0167729             0.0370432   \n",
       "39          0.378734               0.347174              0.284027   \n",
       "17         0.0916898              0.0067629             0.0352466   \n",
       "38          0.330353               0.149633              0.182421   \n",
       "\n",
       "   test_matthews_corrcoef_mean test_matthews_corrcoef_std test_roc_auc_mean  \\\n",
       "41                    0.671749                  0.0313675          0.826875   \n",
       "28                    0.568289                  0.0487082          0.774974   \n",
       "30                    0.632775                 0.00743012          0.812709   \n",
       "35                    0.671717                  0.0625067           0.83006   \n",
       "31                    0.636273                  0.0241465           0.81485   \n",
       "29                    0.641114                 0.00511518          0.817939   \n",
       "1                     0.689048                  0.0197066          0.844484   \n",
       "0                     0.663726                   0.017619          0.831661   \n",
       "4                     0.699015                  0.0241805          0.849242   \n",
       "14                    0.698716                   0.018907          0.848965   \n",
       "9                       0.6564                  0.0179992          0.828219   \n",
       "8                     0.673457                  0.0326426          0.836641   \n",
       "40                    0.595043                  0.0827983          0.790264   \n",
       "5                     0.712728                  0.0298079          0.854735   \n",
       "13                    0.667583                  0.0159159          0.833525   \n",
       "7                     0.646058                  0.0104081          0.822276   \n",
       "16                    0.616535                  0.0163255          0.807483   \n",
       "33                    0.455309                  0.0597158          0.708315   \n",
       "23                    0.619185                  0.0141174          0.808713   \n",
       "37                    0.487356                   0.277744          0.739156   \n",
       "18                    0.610406                  0.0286675          0.803649   \n",
       "19                    0.610064                   0.010239          0.803021   \n",
       "24                    0.584084                  0.0256784          0.791131   \n",
       "21                    0.579067                  0.0197852          0.788653   \n",
       "11                    0.548231                  0.0294851          0.773892   \n",
       "3                     0.590584                  0.0403927          0.792905   \n",
       "26                    0.546003                  0.0175165          0.772624   \n",
       "2                     0.456508                  0.0278473          0.728197   \n",
       "6                     0.442264                  0.0177146          0.720811   \n",
       "32                    0.386408                   0.170787          0.690116   \n",
       "36                    0.536624                   0.269682          0.764958   \n",
       "44                    0.448446                   0.233283          0.704474   \n",
       "10                    0.402688                 0.00363142          0.701117   \n",
       "15                    0.346415                    0.01985          0.672474   \n",
       "25                     0.37654                  0.0156226          0.686204   \n",
       "34                    0.457865                   0.229394          0.723261   \n",
       "20                    0.360414                  0.0327149          0.670308   \n",
       "42                    0.293217                    0.21273          0.637067   \n",
       "22                   0.0645503                  0.0142397          0.532194   \n",
       "27                   0.0338504                  0.0285656          0.516931   \n",
       "12                   0.0168308                  0.0370997          0.508376   \n",
       "39                    0.355054                   0.290722          0.674206   \n",
       "17                  0.00598898                  0.0368196          0.503311   \n",
       "38                    0.162689                   0.186538          0.574962   \n",
       "\n",
       "   test_roc_auc_std  \n",
       "41        0.0225728  \n",
       "28         0.029947  \n",
       "30       0.00612512  \n",
       "35        0.0401371  \n",
       "31        0.0151005  \n",
       "29       0.00214471  \n",
       "1        0.00987974  \n",
       "0        0.00888021  \n",
       "4         0.0119993  \n",
       "14       0.00977927  \n",
       "9        0.00899646  \n",
       "8         0.0163817  \n",
       "40        0.0440962  \n",
       "5         0.0152261  \n",
       "13       0.00815279  \n",
       "7        0.00520515  \n",
       "16       0.00812435  \n",
       "33        0.0426989  \n",
       "23       0.00732536  \n",
       "37         0.140914  \n",
       "18        0.0143071  \n",
       "19       0.00589433  \n",
       "24        0.0132908  \n",
       "21        0.0095421  \n",
       "11        0.0145199  \n",
       "3          0.020566  \n",
       "26       0.00873717  \n",
       "2         0.0139397  \n",
       "6        0.00901459  \n",
       "32        0.0877003  \n",
       "36         0.133473  \n",
       "44         0.110685  \n",
       "10       0.00167203  \n",
       "15       0.00973502  \n",
       "25       0.00799347  \n",
       "34         0.112165  \n",
       "20        0.0174249  \n",
       "42         0.108846  \n",
       "22       0.00708121  \n",
       "27        0.0142689  \n",
       "12        0.0185061  \n",
       "39         0.142535  \n",
       "17        0.0176497  \n",
       "38        0.0913936  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[[ \"Model\",\"test_acc_mean\",\"test_acc_std\", \n",
    "                        \"test_balanced_accuracy_mean\",\"test_balanced_accuracy_std\", \n",
    "                       \"test_prec_mean\", \"test_prec_std\", \n",
    "                        \"test_recall_mean\",\"test_recall_std\", \n",
    "                       \"test_f1-score_mean\", \"test_f1-score_std\", \n",
    "                       \"test_cohens_kappa_mean\", \"test_cohens_kappa_std\", \"test_matthews_corrcoef_mean\",\"test_matthews_corrcoef_std\", \n",
    "                       \"test_roc_auc_mean\", \"test_roc_auc_std\"]][df_results[\"test_prec_mean\"]<1].sort_values(by=[\"test_prec_mean\", \"test_recall_mean\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    df_results.sort_values(by=[\"test_prec_mean\", \"test_recall_mean\"], ascending=False).to_csv(\"model_selection_results_IMDB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
